{
 "cells": [
  {
   "cell_type": "raw",
   "id": "dee15505",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "format:\n",
    "  html:\n",
    "    theme: cosmo\n",
    "    linkcolor: red\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae7b99c-29bd-42be-b041-d59f2d1058b8",
   "metadata": {},
   "source": [
    "This is the notes for the [Advanced Web Scraping - with AI flavor!](https://www.eventbrite.com/e/advanced-web-scraping-with-ai-flavor-lede-2025-info-session-tickets-1220984580749?aff=oddtdtcreator) session, which was a sample class and info session hosted by [Professor Jonathan Soma](https://jonathansoma.com/) for the [Lede Program](https://ledeprogram.com/), a summer data journalism intensive at Columbia Journalism School.\n",
    "\n",
    "In this session we'll learn to use [Playwright](https://playwright.dev/python/) along with a particular [AI prompt](https://raw.githubusercontent.com/jsoma/scraping-ai-flavor/refs/heads/main/prompt.md) to write a scraper.\n",
    "\n",
    "## Requests and BeautifulSoup intro\n",
    "\n",
    "The traditional entry point for learning to scrape in Python is by using [requests](https://pypi.org/project/requests/) and [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/). It's usually great!\n",
    "\n",
    "In the case below, we're using it to scrape headlines [Le Monde's English website](https://www.lemonde.fr/en/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296de6ed-ff06-4300-9c70-37fae4faca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://www.lemonde.fr/en/\")\n",
    "doc = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac576e0-954c-4e00-af8f-d2d0e096df89",
   "metadata": {},
   "source": [
    "Sometimes you'll get lucky and be able to scrape by just specifying a tag name..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a09d1-1227-47ed-89dd-032a025210e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = doc.find_all('h3')\n",
    "for headline in headlines:\n",
    "    print(headline.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da344c5-3504-4248-a044-06da426e2f32",
   "metadata": {},
   "source": [
    "...but more often that not a class is going to be more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c8243-c747-4f8c-a438-b497341c190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = doc.find_all(class_='article__title')\n",
    "for headline in headlines:\n",
    "    print(headline.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77391c43-7cdd-433f-85c8-77ccf4c17c2a",
   "metadata": {},
   "source": [
    "## Where requests + BeautifulSoup fails\n",
    "\n",
    "Some websites you'll be able to download fine with requests, but when you start trying to use BeautifulSoup *nothing shows up*. For example, if we try to access [OpenSyllabus listing pages](https://analytics.opensyllabus.org/record/works) we won't see any books show up in BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f7531-66c0-4615-bc06-6984d67bb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://analytics.opensyllabus.org/record/works\")\n",
    "doc = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0edcd3-d4a3-4720-a9e7-76bf75f88a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.find_all(class_='fOVKMS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e98aa-a909-41a3-bcfb-1625152bc8c2",
   "metadata": {},
   "source": [
    "This is because the page retrived by requests *doesn't actually have all those books on it*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e6277-8d24-4f69-a6cb-5c3165f89d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0baa84-2109-442b-b869-a33d0e4974a2",
   "metadata": {},
   "source": [
    "This is because visiting this site is a two-step process, first you load up this bare-bones skeleton page, then **the browser goes and gets the actual information.** Requests doesn't do that next step, so we need to try another tool!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f683a1-d6f1-453e-8b4d-41540791c0af",
   "metadata": {},
   "source": [
    "## Enter Playwright\n",
    "\n",
    "Instead of pulling the raw HTML contents of the page, Playwright actually controls your browser for you! It can load pages up, you can click things, fill out forms, all sorts of things. To begin we'll just access the same OpenSyllabus page as before and see the actual contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d9c00-7cb9-4cc0-aed4-0c849ee3e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# \"Hey, open up a browser\"\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "\n",
    "# Create a new browser window\n",
    "page = await browser.new_page()\n",
    "\n",
    "# Tell it to go to this page\n",
    "await page.goto(\"https://analytics.opensyllabus.org/record/works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba3e675-8434-45f3-adb5-af5c4f485287",
   "metadata": {},
   "source": [
    "Some people will actually scrape the page Playwright, grabbing titles and all of that, but I find it's easiest to take the HTML – the *full* HTML, after the skeleton has been filled in – and feed it to BeautifulSoup, just like we're used to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cb9f8-f2aa-40fd-bdc3-593cfff53b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = await page.content()\n",
    "\n",
    "doc = BeautifulSoup(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f68679a-c96e-4cae-bfb0-bc1f8b396a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.find_all(class_='fOVKMS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb21f8-4631-4c54-adb4-9e4777d79f5f",
   "metadata": {},
   "source": [
    "Now that we know how to access the page, we can grab the content just like we'd do with a \"normal\" requests/BeautifulSoup page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6ad91c-511e-4805-b06c-b782b5d115ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87231e3-1ffe-47fa-b655-ec5a1dfe2ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bf3a5f7-cf7c-47ab-bafc-f5b4edb848c8",
   "metadata": {},
   "source": [
    "And then we can do all anyone ever wants to do, which is convert it into a spreadsheet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ddb0f6-096b-4aa3-97cd-91b2319d3535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffff8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e87c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90be0f08-2c26-4935-a2f0-942d1ba1439d",
   "metadata": {},
   "source": [
    "## Interacting with the page\n",
    "\n",
    "If we scroll down a bit, we see that the page **only lists the top 50 books**. We want more than that! And we get that by clicking the \"Show More\" button.\n",
    "\n",
    "Playwright makes it easy with `page.get_by_text` and `.click()` – but instead of writing the code ourselves, we're just going to get ChatGPT (or Claude, or Deepseek...) to write the code for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d4245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33924826-c9e7-4fec-9268-7a950048c885",
   "metadata": {},
   "source": [
    "## Filling out forms\n",
    "\n",
    "Let's try another page where we need to fill out some forms. The [North Dakota well search page](https://www.dmr.nd.gov/oilgas/findwellsvw.asp) is a good one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c0a14-4f7d-46b4-b4da-4adc8a6ce0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12495b9a-b340-40d5-a1a2-18b64b92e726",
   "metadata": {},
   "source": [
    "Selecting from dropdowns is easy! But again, we don't need to know how to do it: we'll just use the prompt and be guided by the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d1997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
